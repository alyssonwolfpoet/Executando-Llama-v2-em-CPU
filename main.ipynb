{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in ./.venv/lib/python3.10/site-packages (0.2.79)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./.venv/lib/python3.10/site-packages (from llama-cpp-python) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./.venv/lib/python3.10/site-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in ./.venv/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in ./.venv/lib/python3.10/site-packages (from llama-cpp-python) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gguf_init_from_file: invalid magic characters 'tjgg'\n",
      "llama_model_load: error loading model: llama_model_loader: failed to load model from models/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "\n",
      "llama_load_model_from_file: failed to load model\n",
      "Exception ignored in: <function Llama.__del__ at 0x714d46ab5990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1972, in __del__\n",
      "    self.close()\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1969, in close\n",
      "    self._stack.close()\n",
      "AttributeError: 'Llama' object has no attribute '_stack'\n",
      "Exception ignored in: <function Llama.__del__ at 0x714d46ab5990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1972, in __del__\n",
      "    self.close()\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1969, in close\n",
      "    self._stack.close()\n",
      "AttributeError: 'Llama' object has no attribute '_stack'\n",
      "Exception ignored in: <function Llama.__del__ at 0x714d46ab5990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1972, in __del__\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1969, in close\n",
      "AttributeError: 'Llama' object has no attribute '_stack'\n",
      "Exception ignored in: <function Llama.__del__ at 0x714d46ab5990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1972, in __del__\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1969, in close\n",
      "AttributeError: 'Llama' object has no attribute '_stack'\n",
      "Exception ignored in: <function Llama.__del__ at 0x714d46ab5990>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1972, in __del__\n",
      "  File \"/home/alysson/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py\", line 1969, in close\n",
      "AttributeError: 'Llama' object has no attribute '_stack'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: models/llama-2-7b-chat.ggmlv3.q4_0.bin",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/llama-2-7b-chat.ggmlv3.q4_0.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/llama.py:358\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[0;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mExitStack()\n\u001b[0;32m--> 358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack\u001b[38;5;241m.\u001b[39menter_context(contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43m_LlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer_ \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;129;01mor\u001b[39;00m LlamaTokenizer(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/Introdução ao Uso da Classe LLM no LangChain/.venv/lib/python3.10/site-packages/llama_cpp/_internals.py:54\u001b[0m, in \u001b[0;36m_LlamaModel.__init__\u001b[0;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_load_model_from_file(\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_model\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfree_model\u001b[39m():\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to load model from file: models/llama-2-7b-chat.ggmlv3.q4_0.bin"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"models/llama-2-7b-chat.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    'models/llama-2-7b-chat.ggmlv3.q4_0.bin',\n",
    "     model_type='llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm = CTransformers(model=\"models/llama-2-7b-chat.ggmlv3.q4_0.bin\", model_type='llama')\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "response = llm_chain.run(\"Por que o céu é azul?\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
